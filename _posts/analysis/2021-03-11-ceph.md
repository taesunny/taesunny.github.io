---
title:  "Ceph"
excerpt: ""

categories:
  - analysis
tags:
  - [ceph]

toc: true
toc_sticky: true
 
date: 2021-03-11
last_modified_at: 2021-03-11
---

# Intro

*Cephalopod(뜻. 두족류 동물, 문어, 오징어 등)의 약자인 Ceph는 로고에 Cephalopod이 표현 되어 있다.*

{:refdef: style="text-align: center;"}
![Ceph logo](/assets/img/analysis/2021-03-11-20-26-54.png)
{: refdef}

Ceph는 분산 storage를 제공하는 Open Source Software Storage Platform이다. 단일 시스템에서 Object Storage, Block Storage, File Storage를 제공하며 안정성이 높고, 쉽게 관리가 가능하다.

---

# Ceph Storage Cluster Architecture

Ceph Storage Cluster에는 여러 Node들이 포함되어, 각 Node의 데몬들이 서로 통신하며 데이터들을 관리한다. Node의 Architecture는 아래의 그림과 같다. ([이미지 출처](https://docs.ceph.com/en/latest/architecture/))

{:refdef: style="text-align: center;"}
![Ceph Architecture](/assets/img/analysis/2021-03-11-16-13-53.png)
{: refdef}

Ceph는 `RADOS`를 기반으로 scalable한 Ceph Storage Cluster를 제공한다. RADOS는 Reliable Automatic Distributed Object Store의 약자로, Ceph의 데이터 관리 기반 기능들을 제공한다.

RADOS는 기본적으로 OSD, MON 데몬으로 이루어 지며, 추가 Storage Type 지원 목적에 따라 MDS와 RGW를 추가적으로 사용한다.

`LIBRADOS`는 Application에서 RADOS에 직접 접근하기 위해 사용할 수 있는 라이브러리이다.

## Daemons

Ceph Storage Cluster는 다음의 daemon들로 이루어져 있다.

### Monitors

Ceph Monitor('ceph-mon')는 cluster state의 map들을 관리한다. map들은 Ceph 데몬들이 서로 협력하기 위한 cluster의 중요한 state 정보들을 관리한다. 또한 Ceph Monitor는 daemon들과 client들 사이의 인증을 관리한다. high-availability를 위해 최소 '세 개'의 Ceph Monitor들이 권장 되고, 홀수개로 구성해야 한다.

- monitor map
- manager map
- OSD map
- MDS map
- CRUSH map

### Managers

Ceph Manager Daemon('ceph-mgr')은 runtime 매트릭들, storage 가용률, performance, system 부하 등의 Ceph Cluster 상태들을 관리한다. 또한 Ceph cluster 정보를 Dashboard UI와 REST API를 통해 제공하기 위해 python-based module을 포함한다. high-availability를 위해 최소 '두 개'의 Manager들이 권장된다.

### OSDs

OSD('ceph-osd')는 Object Storage Daemon의 약자로, data들의 replication, recovery, rebalancing의 역할을 하며, 다른 Ceph OSD daemon 사이에 heartbeat를 주고 받으며 Ceph Monitor들과 Manager들에게 정보를 제공한다. high-availability를 위해 최소 '세 개'의 OSD들이 권장된다.

### MDSs

MDS('ceph-mds')는 Ceph Metadata Server의 약자로, Ceph File System를 위해(Block, Object Storage는 사용하지 않음) metadata를 저장한다. MDS는 ls, find와 같은 POSIX file system의 기본 명령들을 제공한다.

## Storage Types

### Object Storage - RADOSGW

Amazon S3와 Openstack Swift와 호환 되는 Bucket기반의 Object Storage를 제공한다. REST API를 통해 Object Storage를 사용할 수 있다.

### Block Device - RBD

RBD는 Rados Block Device의 약자로, 스냅샷과 복제 기능을 제공하는 Block Device 기능을 제공한다.

### File System - CephFS

MDS를 사용하여 파일 시스템으로 사용가능한 POSIX 호환 파일 시스템을 제공한다.

---

# 기타 개념

## Placement Group

Object들의 Logical한 단위이다. CRUSH 알고리즘을 통해 Object가 관리될 Placement Group이 선택되고, 해당 Placement Group에 속한 OSD가 해당 Object를 관리한다.

## CRUSH Algorithm

CRUSH는 Controlled Replication Un- der Scalable Hashing의 약자로, CRUSH 알고리즘을 이용하여 Logical Storage Pool 내부에 object들을 저장한다. CRUSH 알고리즘을 통해 object들을 저장할 PG(placement group)와 PG를 저장할 OSD Daemon이 계산된다. CRUSH 알고리즘으로 Ceph Storage Cluster의 scaling, rebalancing, recovery를 효율적으로 처리한다.

## Pool

Ceph의 object를 Logical Partition을 의미한다. 각 Pool은 Object Storage, Block Device와 같은 Storage Type을 가진다.

- pool 단위로 스냅샷이 가능하다.
- 데이터 손실을 방지하기 위해 replica 개수를 설정할 수 있다.
- Pool의 Placement Group의 개수를 설정할 수 있다. (Default: OSD당 약 100개)

---

# Source

- [https://en.wikipedia.org/wiki/Ceph_(software)](https://en.wikipedia.org/wiki/Ceph_(software))
- [https://docs.ceph.com/en/latest/start/intro/](https://docs.ceph.com/en/latest/start/intro/)
- [https://docs.ceph.com/en/latest/architecture/](https://docs.ceph.com/en/latest/architecture/)
- [https://docs.ceph.com/en/latest/rados/operations/pools/](https://docs.ceph.com/en/latest/rados/operations/pools/)
- [로고 출처](https://ceph.io/)